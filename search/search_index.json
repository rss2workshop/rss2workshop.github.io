{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"RSS2: Workshop on Robustness and Safe Software 2.0 This workshop is in conjunction with ASPLOS 2021 Friday, April 16, 2021 | Full Day Introduction Welcome to the Workshop on Robustness and Safe Software 2.0 (RSS2). Unlike Software 1.0 (conventional programs) that is manually coded with hardened parameters and explicit logics, Software 2.0 programs, usually manifested as and enabled by Deep Neural Networks (DNN), have learnt parameters and implicit logics. Software 2.0 is found in a diverse set of applications in today\u2019s society, ranging from autonomous machines, Augmented/Virtual Reality devices, to smart-city infrastructures. While the systems and architecture communities have focused, rightly so, on the efficiency of DNNs, Software 2.0 exposes a unique set of challenges for robustness, safety, and resiliency, which are major roadblocks before Software 2.0 becomes a pervasive computing paradigm. For instance, small perturbations to inputs could easily \u201cfool\u201d DNNs to produce incorrect results, giving rise to the so-called adversarial attacks. Similarly, while DNNs are generally resilient to hardware faults, few have studied the worst-case resiliency of DNNs to hardware faults, which usually dictated the safety of mission-critical systems. Improving the robustness, safety, and resiliency of Software 2.0 is necessarily a cross-layer task, just like how algorithms, programming language, architecture, and circuits communities came together in the Software 1.0 era. It is also critical to not hyper-optimize individual system components; rather, we must take a whole-of-system approach that understands the requirements and constraints of end-to-end systems, which are usually multi-chip and span both client, edge, and cloud. To that end, the workshop is meant to foster an interactive discussion about computer systems and architecture research\u2019s role of robust, safe, and resilient Software 2.0. Ultimately, the workshop is meant to lead to new discussions and insights on algorithms, architectures, and circuit/device-level design as well as system-level integration and co-design. Organizer Yuhao Zhu Priya Panda Agenda (All Times are EST) Opening and Welcome : 12:00 ~ 12:10 Session 1: Algorithms & Applications: What are the new challenges for robustness, safety and resiliency? Time Speaker Content 12:10 ~ 12:35 Alfred Chen (UC Irvine) Towards Secure and Robust Autonomy Software in Autonomous Driving 12:35 ~ 13:00 Helen Li (Duke University) Advancing the Design of Adversarial Machine Learning Methods 13:00 ~ 13:25 Hima Lakkaraju (Havard University) Topic 3 13:25 ~ 13:45 Panel 13:45 ~ 13:55 Break Session 2: Architecture & Systems: How can we build resilient and safe systems and hardware? Time Speaker Content 13:55 ~ 14:20 Yanjing Li (University of Chicago) Resilient Deep Learning Accelerators 14:20 ~ 14:45 Farinaz Koushanfar (UC San Diego) Topic 2 14:45 ~ 15:10 Anand Raghunathan (Purdue University) Topic 3 15:10 ~ 15:30 Panel 15:30 ~ 15:40 Break Session 3: Circuits & Devices: How can we harness emerging devices and their characteristics for extreme robustness? Time Speaker Content 15:40 ~ 16:05 Kaushik Roy (Purdue University) Topic 1 16:05 ~ 16:30 Suman Datta (University of Notre Dame) Topic 2 16:30 ~ 16:55 Shimeng Yu (Georgia Institute of Technology) Topic 3 16:55 ~ 17:15 Panel Closing: 17:15 ~ 17:25 Talk 1: Towards Secure and Robust Autonomy Software in Autonomous Driving Abstract: Autonomous Driving (AD) technology has always been an international pursuit due to its significant benefit in driving safety, efficiency, and mobility. Over 15 years after the first DARPA Grand Challenge, its development and deployment are becoming increasingly mature and practical, with some AD vehicles already providing services on public roads (e.g., Google Waymo One in Phoenix and Baidu Apollo Go in China). In AD technology, the autonomy software stack, or the AD software, is highly security critical: it is in charge of safety-critical driving decisions such as collision avoidance and lane keeping, and thus any security problems in it can directly impact road safety. In this talk, I will describe my recent research that initiates the first systematic effort towards understanding and addressing the security problems in production AD software. I will be focusing on two critical modules: perception and localization, and talk about how we are able to discover novel and practical sensor/physical-world attacks that can cause end-to-end safety impacts such as crashing into obstacles or driving off road. Besides AD software, I will also briefly talk about my recent research on autonomy software security in smart transportation in general, especially those enabled by Connected Vehicle (CV) technology. I will conclude with a discussion on defense and future research directions. Speaker Bio: Qi Alfred Chen is an Assistant Professor in the Department of Computer Science at the University of California, Irvine. His research interest spans software security, systems security, and network security. Currently, his research focuses on security problems in autonomous CPS and IoT systems (e.g., autonomous driving and intelligent transportation). His works have high impacts in both academic and industry with over 30 research papers in top-tier venues in security, mobile systems, transportation, software engineering, and machine learning; a nationwide USDHS US-CERT alert, and multiple CVEs; over 50 news articles by major news media such as Forbes, Fortune, and BBC News; and vulnerability report acknowledgments from USDOT, Apple, Microsoft, Comcast, Daimler, etc. Recently, his research triggered over 20 autonomous driving companies such as Tesla, GM, Baidu, and Daimler to start security vulnerability investigations; some confirmed to work on fixes. He serves as reviewers for various top-tier venues such as Usenix Security, ACM CCS, TIFS, TDSC, T-ITS, etc., and co-found the AutoSec workshop (co-located with NDSS\u201921). His group won the 1st place in the first AutoDriving Security CTF (part of BCTF) in 2020. Chen received his Ph.D. from the University of Michigan in 2018. Talk 2: Advancing the Design of Adversarial Machine Learning Methods Abstract: It has become clear that deep neural networks (DNNs) have an immense potential to learn and perform complex tasks. It is also evident that DNNs have many vulnerabilities with the potential to render them useless in complex and extended operating environments. The purpose of our research is to investigate ways in which DNN models are vulnerable to \u201cadversarial attacks,\u201d while also leveraging such adversarial techniques to construct more robust and reliable deep learning-based systems. We explore the potential weaknesses of DNN models by developing advancedfeature space-based adversarial attacks, which create adversarial directions that are generally effective for data distribution. The learned distributions can also be used to analyze layer-wise and model-wise transfer properties and gain insights into how feature distributions evolve with layer depth and architecture. Alternatively, we investigate the ensemble methods against transfer attacks. Our approach (namely, DVERGE) isolates the adversarial vulnerability in each sub-model by distilling non-robust features. It then diversifies the adversarial vulnerability to induce diverse outputs against a transfer attack. New challenges for developing robust DNN models will be discussed at the end of the talk. Speaker Bio: Hai \u201cHelen\u201d Li is Clare Boothe Luce Professor and Associate Chair of the Department of Electrical and Computer Engineering at Duke University. She received her B.S and M.S. from Tsinghua University and Ph.D. from Purdue University. At Duke, she co-directs Duke University Center for Computational Evolutionary Intelligence and NSF IUCRC for Alternative Sustainable and Intelligent Computing (ASIC). Her research interests include machine learning acceleration and security, neuromorphic circuits and systems for brain-inspired computing, conventional and emerging memory, and software and hardware co-design. She received the NSF CAREER Award, the DARPA Young Faculty Award, TUM-IAS Hans Fischer Fellowship from Germany, ELATE Fellowship, night best paper awards, and another nine best paper nominations. Dr. Li is a fellow of IEEE and a distinguished member of ACM. For more information, please see her webpage at here . Talk 3: Resilient Deep Learning Accelerators Abstract: Resilience to hardware failures is a key challenge as well as a top priority for deep learning (DL) accelerators, which have been deployed in a wide range of application domains, from edge computing, self-driving cars, to cloud servers. DL accelerates are susceptible to various hardware failure sources, including temporary/transient errors (such as soft errors and dynamic variations) and permanent failures (such as early-life failures, circuit aging, and manufacturing defect escapes). Although DL workloads exhibit certain tolerance to errors, such tolerance alone cannot guarantee that a DL accelerator will meet the resilience requirement of a target application in the presence of hardware errors. In this talk, I will first present a resilience analysis framework, which takes advantage of the architectural properties of DL accelerators to accurately and quickly analyze the behavior of hardware errors in these accelerators. Next, using this framework, we perform a large-scale resilience study to thoroughly understand the resilience properties of DL accelerators/workloads. The key findings of this study will be discussed. Finally, I will share our insights on how to design resilient DL accelerators. Speaker Bio: Yanjing Li is an Assistant Professor in the Department of Computer Science (Systems Group) at the University of Chicago. Prior to joining University of Chicago, she was a senior research scientist at Intel Labs. Professor Li received her Ph.D. in Electrical Engineering from Stanford University, a M.S. in Mathematical Sciences (with honors) and a B.S. in Electrical and Computer Engineering (with a double major in Computer Science) from Carnegie Mellon University. Professor Li has received various awards, including Google research scholar award, NSF/SRC energy-efficient computing: from devices to architectures (E2CDA) program award, Intel Labs Gordy academy award (highest honor in Intel Labs) and several other Intel recognition awards, outstanding dissertation award (European Design and Automation Association), and multiple best paper awards (ACM Great Lakes Symposium on VLSI, and IEEE VLSI Test Symposium, and IEEE International Test Conference).","title":"RSS2: Workshop on Robustness and Safe Software 2.0"},{"location":"#rss2-workshop-on-robustness-and-safe-software-20","text":"This workshop is in conjunction with ASPLOS 2021 Friday, April 16, 2021 | Full Day","title":"RSS2: Workshop on Robustness and Safe Software 2.0"},{"location":"#introduction","text":"Welcome to the Workshop on Robustness and Safe Software 2.0 (RSS2). Unlike Software 1.0 (conventional programs) that is manually coded with hardened parameters and explicit logics, Software 2.0 programs, usually manifested as and enabled by Deep Neural Networks (DNN), have learnt parameters and implicit logics. Software 2.0 is found in a diverse set of applications in today\u2019s society, ranging from autonomous machines, Augmented/Virtual Reality devices, to smart-city infrastructures. While the systems and architecture communities have focused, rightly so, on the efficiency of DNNs, Software 2.0 exposes a unique set of challenges for robustness, safety, and resiliency, which are major roadblocks before Software 2.0 becomes a pervasive computing paradigm. For instance, small perturbations to inputs could easily \u201cfool\u201d DNNs to produce incorrect results, giving rise to the so-called adversarial attacks. Similarly, while DNNs are generally resilient to hardware faults, few have studied the worst-case resiliency of DNNs to hardware faults, which usually dictated the safety of mission-critical systems. Improving the robustness, safety, and resiliency of Software 2.0 is necessarily a cross-layer task, just like how algorithms, programming language, architecture, and circuits communities came together in the Software 1.0 era. It is also critical to not hyper-optimize individual system components; rather, we must take a whole-of-system approach that understands the requirements and constraints of end-to-end systems, which are usually multi-chip and span both client, edge, and cloud. To that end, the workshop is meant to foster an interactive discussion about computer systems and architecture research\u2019s role of robust, safe, and resilient Software 2.0. Ultimately, the workshop is meant to lead to new discussions and insights on algorithms, architectures, and circuit/device-level design as well as system-level integration and co-design.","title":"Introduction"},{"location":"#organizer","text":"Yuhao Zhu Priya Panda","title":"Organizer"},{"location":"#agenda-all-times-are-est","text":"","title":"Agenda (All Times are EST)"},{"location":"#opening-and-welcome-1200-1210","text":"","title":"Opening and Welcome : 12:00 ~ 12:10"},{"location":"#session-1-algorithms-applications-what-are-the-new-challenges-for-robustness-safety-and-resiliency","text":"Time Speaker Content 12:10 ~ 12:35 Alfred Chen (UC Irvine) Towards Secure and Robust Autonomy Software in Autonomous Driving 12:35 ~ 13:00 Helen Li (Duke University) Advancing the Design of Adversarial Machine Learning Methods 13:00 ~ 13:25 Hima Lakkaraju (Havard University) Topic 3 13:25 ~ 13:45 Panel 13:45 ~ 13:55 Break","title":"Session 1: Algorithms &amp; Applications: What are the new challenges for robustness, safety and resiliency?"},{"location":"#session-2-architecture-systems-how-can-we-build-resilient-and-safe-systems-and-hardware","text":"Time Speaker Content 13:55 ~ 14:20 Yanjing Li (University of Chicago) Resilient Deep Learning Accelerators 14:20 ~ 14:45 Farinaz Koushanfar (UC San Diego) Topic 2 14:45 ~ 15:10 Anand Raghunathan (Purdue University) Topic 3 15:10 ~ 15:30 Panel 15:30 ~ 15:40 Break","title":"Session 2: Architecture &amp; Systems: How can we build resilient and safe systems and hardware?"},{"location":"#session-3-circuits-devices-how-can-we-harness-emerging-devices-and-their-characteristics-for-extreme-robustness","text":"Time Speaker Content 15:40 ~ 16:05 Kaushik Roy (Purdue University) Topic 1 16:05 ~ 16:30 Suman Datta (University of Notre Dame) Topic 2 16:30 ~ 16:55 Shimeng Yu (Georgia Institute of Technology) Topic 3 16:55 ~ 17:15 Panel","title":"Session 3: Circuits &amp; Devices: How can we harness emerging devices and their characteristics for extreme robustness?"},{"location":"#closing-1715-1725","text":"","title":"Closing: 17:15 ~ 17:25"},{"location":"#talk-1-towards-secure-and-robust-autonomy-software-in-autonomous-driving","text":"Abstract: Autonomous Driving (AD) technology has always been an international pursuit due to its significant benefit in driving safety, efficiency, and mobility. Over 15 years after the first DARPA Grand Challenge, its development and deployment are becoming increasingly mature and practical, with some AD vehicles already providing services on public roads (e.g., Google Waymo One in Phoenix and Baidu Apollo Go in China). In AD technology, the autonomy software stack, or the AD software, is highly security critical: it is in charge of safety-critical driving decisions such as collision avoidance and lane keeping, and thus any security problems in it can directly impact road safety. In this talk, I will describe my recent research that initiates the first systematic effort towards understanding and addressing the security problems in production AD software. I will be focusing on two critical modules: perception and localization, and talk about how we are able to discover novel and practical sensor/physical-world attacks that can cause end-to-end safety impacts such as crashing into obstacles or driving off road. Besides AD software, I will also briefly talk about my recent research on autonomy software security in smart transportation in general, especially those enabled by Connected Vehicle (CV) technology. I will conclude with a discussion on defense and future research directions. Speaker Bio: Qi Alfred Chen is an Assistant Professor in the Department of Computer Science at the University of California, Irvine. His research interest spans software security, systems security, and network security. Currently, his research focuses on security problems in autonomous CPS and IoT systems (e.g., autonomous driving and intelligent transportation). His works have high impacts in both academic and industry with over 30 research papers in top-tier venues in security, mobile systems, transportation, software engineering, and machine learning; a nationwide USDHS US-CERT alert, and multiple CVEs; over 50 news articles by major news media such as Forbes, Fortune, and BBC News; and vulnerability report acknowledgments from USDOT, Apple, Microsoft, Comcast, Daimler, etc. Recently, his research triggered over 20 autonomous driving companies such as Tesla, GM, Baidu, and Daimler to start security vulnerability investigations; some confirmed to work on fixes. He serves as reviewers for various top-tier venues such as Usenix Security, ACM CCS, TIFS, TDSC, T-ITS, etc., and co-found the AutoSec workshop (co-located with NDSS\u201921). His group won the 1st place in the first AutoDriving Security CTF (part of BCTF) in 2020. Chen received his Ph.D. from the University of Michigan in 2018.","title":"Talk 1: Towards Secure and Robust Autonomy Software in Autonomous Driving"},{"location":"#talk-2-advancing-the-design-of-adversarial-machine-learning-methods","text":"Abstract: It has become clear that deep neural networks (DNNs) have an immense potential to learn and perform complex tasks. It is also evident that DNNs have many vulnerabilities with the potential to render them useless in complex and extended operating environments. The purpose of our research is to investigate ways in which DNN models are vulnerable to \u201cadversarial attacks,\u201d while also leveraging such adversarial techniques to construct more robust and reliable deep learning-based systems. We explore the potential weaknesses of DNN models by developing advancedfeature space-based adversarial attacks, which create adversarial directions that are generally effective for data distribution. The learned distributions can also be used to analyze layer-wise and model-wise transfer properties and gain insights into how feature distributions evolve with layer depth and architecture. Alternatively, we investigate the ensemble methods against transfer attacks. Our approach (namely, DVERGE) isolates the adversarial vulnerability in each sub-model by distilling non-robust features. It then diversifies the adversarial vulnerability to induce diverse outputs against a transfer attack. New challenges for developing robust DNN models will be discussed at the end of the talk. Speaker Bio: Hai \u201cHelen\u201d Li is Clare Boothe Luce Professor and Associate Chair of the Department of Electrical and Computer Engineering at Duke University. She received her B.S and M.S. from Tsinghua University and Ph.D. from Purdue University. At Duke, she co-directs Duke University Center for Computational Evolutionary Intelligence and NSF IUCRC for Alternative Sustainable and Intelligent Computing (ASIC). Her research interests include machine learning acceleration and security, neuromorphic circuits and systems for brain-inspired computing, conventional and emerging memory, and software and hardware co-design. She received the NSF CAREER Award, the DARPA Young Faculty Award, TUM-IAS Hans Fischer Fellowship from Germany, ELATE Fellowship, night best paper awards, and another nine best paper nominations. Dr. Li is a fellow of IEEE and a distinguished member of ACM. For more information, please see her webpage at here .","title":"Talk 2: Advancing the Design of Adversarial Machine Learning Methods"},{"location":"#talk-3-resilient-deep-learning-accelerators","text":"Abstract: Resilience to hardware failures is a key challenge as well as a top priority for deep learning (DL) accelerators, which have been deployed in a wide range of application domains, from edge computing, self-driving cars, to cloud servers. DL accelerates are susceptible to various hardware failure sources, including temporary/transient errors (such as soft errors and dynamic variations) and permanent failures (such as early-life failures, circuit aging, and manufacturing defect escapes). Although DL workloads exhibit certain tolerance to errors, such tolerance alone cannot guarantee that a DL accelerator will meet the resilience requirement of a target application in the presence of hardware errors. In this talk, I will first present a resilience analysis framework, which takes advantage of the architectural properties of DL accelerators to accurately and quickly analyze the behavior of hardware errors in these accelerators. Next, using this framework, we perform a large-scale resilience study to thoroughly understand the resilience properties of DL accelerators/workloads. The key findings of this study will be discussed. Finally, I will share our insights on how to design resilient DL accelerators. Speaker Bio: Yanjing Li is an Assistant Professor in the Department of Computer Science (Systems Group) at the University of Chicago. Prior to joining University of Chicago, she was a senior research scientist at Intel Labs. Professor Li received her Ph.D. in Electrical Engineering from Stanford University, a M.S. in Mathematical Sciences (with honors) and a B.S. in Electrical and Computer Engineering (with a double major in Computer Science) from Carnegie Mellon University. Professor Li has received various awards, including Google research scholar award, NSF/SRC energy-efficient computing: from devices to architectures (E2CDA) program award, Intel Labs Gordy academy award (highest honor in Intel Labs) and several other Intel recognition awards, outstanding dissertation award (European Design and Automation Association), and multiple best paper awards (ACM Great Lakes Symposium on VLSI, and IEEE VLSI Test Symposium, and IEEE International Test Conference).","title":"Talk 3: Resilient Deep Learning Accelerators"}]}